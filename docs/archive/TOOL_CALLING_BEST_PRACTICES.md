# Tool-Calling Best Practices for Agentic Systems

**Date:** October 2025
**Context:** Redis Wellness Demo - Stateless vs Agentic RAG Chat

## Summary

This document outlines best practices for implementing LLM tool-calling in agentic systems, specifically for demos showcasing autonomous AI behavior.

---

## âœ… Current Implementation (CORRECT)

### 1. **Native LLM Tool Selection**

**What we do:**
```python
# Present ALL tools to the LLM
tools_to_use = user_tools
llm_with_tools = self.llm.bind_tools(tools_to_use)
response = await llm_with_tools.ainvoke(conversation)
```

**Why this is best practice:**
- âœ… Showcases Qwen 2.5's native function-calling capabilities
- âœ… Enables complex multi-step queries with tool chaining
- âœ… More robust than regex-based routing
- âœ… True autonomous agentic behavior for demos

### 2. **Autonomous Tool Loop**

**What we do:**
```python
for iteration in range(max_tool_calls):
    response = await llm_with_tools.ainvoke(conversation)

    if not response.tool_calls:
        break  # LLM decided it's done

    # Execute LLM's chosen tools
    for tool_call in response.tool_calls:
        result = await tool.ainvoke(tool_call["args"])
        conversation.append(ToolMessage(content=result))
```

**Why this is best practice:**
- âœ… LLM decides which tools to call
- âœ… LLM decides when to stop (no tool calls = done)
- âœ… Supports multi-step reasoning (search â†’ aggregate â†’ analyze)
- âœ… Demonstrates true AI autonomy

### 3. **Rich Tool Documentation**

**What we do:**
```python
@tool
def aggregate_metrics(metric_types: list[str], ...) -> dict:
    """
    ðŸ”¢ CALCULATE STATISTICS - Use for mathematical aggregation.

    âš ï¸ USE THIS TOOL WHEN USER ASKS FOR:
    - AVERAGE/MEAN: "average heart rate", "mean weight"
    - MIN/MAX: "minimum", "lowest", "highest"
    - TOTAL/SUM: "total steps", "sum of calories"

    âŒ DO NOT USE THIS TOOL FOR:
    - Individual data points â†’ use search_health_records_by_metric
    - Viewing trends â†’ use search_health_records_by_metric

    Returns: COMPUTED STATISTICS (single numbers), NOT raw data.
    """
```

**Why this is best practice:**
- âœ… Clear use cases help LLM understand tool purpose
- âœ… Negative examples prevent tool misuse
- âœ… Explicit return type clarifies expectations
- âœ… Emojis and formatting improve LLM comprehension

---

## âš ï¸ Anti-Patterns to Avoid

### âŒ **Pre-LLM Tool Filtering**

**What we REMOVED:**
```python
# âŒ DON'T DO THIS (removed from codebase)
classification = self.query_classifier.classify_intent(query)
if classification["confidence"] >= 0.5:
    # Only present "recommended" tools
    tools_to_use = self._filter_tools(user_tools, classification)
```

**Why this is problematic:**
- âŒ Limits multi-step queries ("Compare Sept vs Oct" needs search + aggregate)
- âŒ Regex matching is brittle ("highest heart rate workout" triggers wrong tool)
- âŒ Hides LLM's true capabilities
- âŒ Makes system feel "rule-based" instead of "intelligent"

### âŒ **Hard-Coded Tool Routing**

**What to avoid:**
```python
# âŒ DON'T DO THIS
if "average" in query or "mean" in query:
    return aggregate_metrics(...)
elif "workout" in query:
    return search_workouts(...)
```

**Why this is problematic:**
- âŒ Cannot handle complex queries
- âŒ Keyword collisions ("What's the average workout duration?" â†’ needs both tools)
- âŒ Undermines agentic narrative for demos

---

## ðŸŽ¯ Recommended Architecture

```
User Query
    â†“
[Verbosity Detection] â† Use QueryClassifier here (UI hints only)
    â†“
Present ALL Tools to LLM
    â†“
LLM Native Tool Selection â† Qwen 2.5 decides autonomously
    â†“
Tool Execution Loop (max 8 iterations)
    â†“
[Memory Storage] â† Redis semantic memory (RAG agent only)
    â†“
Response to User
```

**Key Principles:**
1. **Lightweight verbosity detection** â†’ Simple regex, no complex classification
2. **All tools available** â†’ Let LLM choose based on query semantics
3. **Tool loop** â†’ Enable multi-step reasoning
4. **Memory augmentation** â†’ Redis context enhances tool selection

---

## ðŸ“Š Demo-Specific Benefits

### For "Stateless vs RAG" Comparison

**Without pre-filtering (current approach):**
- âœ… Shows true autonomous behavior â†’ "Wow, it chains tools!"
- âœ… Handles unexpected queries â†’ "It understood my complex question!"
- âœ… **Showcases Qwen 2.5** â†’ "This is real AI function calling"
- âœ… **Redis memory as differentiator** â†’ "Stateless can't remember context!"

**With pre-filtering (old approach):**
- âŒ Users think it's "keyword matching" â†’ "Just hard-coded rules?"
- âŒ Complex queries fail â†’ "Why can't it compare periods?"
- âŒ Undermines "agentic" narrative

---

## ðŸ”§ Implementation Guidelines

### Tool Design Checklist

When creating new tools:

1. **Clear Purpose Statement**
   ```python
   """
   USE THIS TOOL WHEN: [specific use cases with examples]
   DO NOT USE FOR: [anti-patterns with alternatives]
   """
   ```

2. **Typed Arguments**
   ```python
   def search_health_records(
       metric_types: list[str],  # Type hints help LLM
       time_period: str = "recent",  # Defaults provide guidance
   ) -> dict[str, Any]:
   ```

3. **Return Format Documentation**
   ```python
   """
   Returns:
       Dict with:
       - results: List of matching records
       - total_found: Integer count
       - time_range: Human-readable period
   """
   ```

4. **Example Usage**
   ```python
   """
   Example:
       Query: "What was my average heart rate last week?"
       â†’ aggregate_metrics(
           metric_types=["HeartRate"],
           time_period="last week",
           aggregations=["average"]
       )
       Returns: {"average": "72.5 bpm", "sample_size": 1250}
   """
   ```

### Agent Configuration

```python
class StatefulRAGAgent:
    def __init__(self, memory_manager):
        self.llm = create_health_llm()  # Qwen 2.5 with low temp (0.05)

    async def chat(self, message: str, ..., max_tool_calls: int = 8):
        # 1. Get all tools (no filtering)
        user_tools = create_user_bound_tools(user_id)

        # 2. Detect verbosity (simple regex - for response style only)
        verbosity = detect_verbosity(message)

        # 3. Present ALL tools to LLM
        llm_with_tools = self.llm.bind_tools(user_tools)

        # 4. Autonomous tool loop
        for iteration in range(max_tool_calls):
            response = await llm_with_tools.ainvoke(conversation)
            if not response.tool_calls:
                break
            # Execute chosen tools...
```

---

## ðŸ§ª Testing Tool Selection

### Validation Queries

Test with queries that require:

1. **Single Tool**
   - "What's my current weight?" â†’ search_health_records

2. **Multiple Tools (Same Call)**
   - "Show me weight, BMI, and heart rate" â†’ search_health_records with multiple metrics

3. **Tool Chaining (Sequential)**
   - "What was my average heart rate last week?" â†’ search_health_records â†’ aggregate_metrics

4. **Complex Multi-Step**
   - "Compare my September vs October activity" â†’
     - search_health_records (September)
     - aggregate_metrics (September)
     - search_health_records (October)
     - aggregate_metrics (October)
     - Final synthesis

5. **Ambiguous Keywords**
   - "Show me my highest heart rate workouts" â†’ Should use search_workouts (NOT aggregate_metrics)
   - "What day do I usually work out?" â†’ search_workouts + pattern analysis

---

## ðŸ“š References

- **LangChain Tool Binding**: https://python.langchain.com/docs/how_to/tool_calling/
- **Qwen 2.5 Function Calling**: https://qwenlm.github.io/blog/qwen2.5/
- **Redis Wellness Demo**: `/docs/INTELLIGENT_HEALTH_TOOLS_PLAN.md`

---

## ðŸ”„ Migration Notes

### Changes Made (October 2025)

**Removed:**
- `_filter_tools()` method from `StatefulRAGAgent`
- Tool pre-filtering based on query classification confidence
- `QueryClassifier` class (replaced with lightweight `detect_verbosity()`)
- `_extract_current_query()` helper method (no longer needed)

**Added:**
- `verbosity_detector.py` - Lightweight verbosity detection (~90 lines vs 300+)
- Simple `detect_verbosity()` function using regex patterns

**Rationale:**
Initial implementation included pre-filtering to "help" the LLM by reducing decision space. Testing revealed this was counterproductive - Qwen 2.5's native tool selection is more robust and enables complex multi-step queries that pre-filtering breaks.

---

## âœ… Review Checklist

Before deploying agentic systems:

- [ ] LLM has access to ALL relevant tools (no pre-filtering)
- [ ] Tool docstrings include clear use cases and anti-patterns
- [ ] Tool loop allows sufficient iterations (8+ for complex queries)
- [ ] Query classification used only for UI hints, not tool restriction
- [ ] Multi-step queries tested (search â†’ aggregate â†’ compare)
- [ ] Error handling doesn't bypass autonomous decision-making
- [ ] Logging shows LLM's tool choices (for demo transparency)
- [ ] Memory augmentation doesn't force specific tool usage

---

**Conclusion:**

The best agentic systems trust the LLM's trained capabilities. Pre-filtering tools or hard-coding routing undermines autonomy and limits functionality. For demos showcasing "intelligence," let the AI be intelligent - native tool selection with rich documentation is the winning pattern.
