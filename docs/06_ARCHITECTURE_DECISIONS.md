# Architecture Decisions: Why These Technologies?

**Teaching Goal:** Understand the reasoning behind Redis Stack, LangGraph, Qwen 2.5 7B, and Ollama - not just what they are, but why they were chosen over alternatives.

## Decision Framework

For each component, we evaluated:

1. **Privacy-first:** Can it run 100% locally?
2. **Developer experience:** How fast can you get started?
3. **Performance:** Does it meet real-time requirements?
4. **Maintainability:** Can one developer understand and extend it?
5. **Cost:** Can it run free forever?

## Decision 1: Why Redis Stack?

### The Choice

**Redis Stack** = Redis + RedisJSON + RedisSearch + RediSearch Vector

### Alternatives Considered

| Option | Pros | Cons | Verdict |
|--------|------|------|---------|
| **PostgreSQL + pgvector** | Mature, relational, ACID | Slow vector search, complex schema, no native caching | ‚ùå Too slow |
| **Pinecone (vector DB)** | Fast vector search | Cloud-only (no privacy), costs money, requires separate DB for data | ‚ùå No privacy |
| **Qdrant (vector DB)** | Local, fast vectors | Requires separate DB for conversations and health data | ‚ùå Too many systems |
| **Redis Stack** | Unified system, in-memory speed, local, RedisVL built-in | Learning curve for Redis data structures | ‚úÖ **Winner** |

### Why Redis Stack Won

#### 1. Unified Memory Store

**Problem:** AI agents need 4 different data types:
- **Conversation history** (short-term memory)
- **Health data** (structured records)
- **Tool patterns** (procedural memory with vector search)
- **Cached embeddings** (avoid re-computing)

**Traditional approach:**
```
PostgreSQL (conversations + health data)
    +
Pinecone (vector search)
    +
Redis (caching)
    = 3 systems to configure, maintain, and troubleshoot
```

**Redis Stack approach:**
```
Redis Stack (all 4 data types)
    = 1 Docker container, 1 configuration file, 1 skill to learn
```

**Code example:**
```python
# One Redis client handles everything
with redis_manager.get_connection() as client:
    # Conversation history (LIST)
    client.lrange("session:abc:messages", 0, 9)

    # Health data (STRING)
    client.get("user:wellness_user:health_data")

    # Workout counts (HASH)
    client.hgetall("user:wellness_user:workout:days")

    # No need to connect to Postgres, then Pinecone, then Redis cache
```

#### 2. RedisVL (Vector Library) Built-In

**RedisVL** = Redis Vector Library with HNSW index support

**What it provides:**
- **HNSW algorithm** for O(log N) vector search (not O(N) brute force)
- **IndexSchema API** for defining vector indexes
- **VectorQuery API** for semantic search
- **Built on Redis** (no separate vector DB needed)

**Alternative: Pinecone**
```python
# Pinecone (requires separate service, cloud account, API key)
import pinecone
pinecone.init(api_key="...", environment="us-west1-gcp")
index = pinecone.Index("procedural-memory")

# Must also maintain Postgres/Redis for non-vector data
# Must sync data between systems
# Must handle network failures to Pinecone
```

**RedisVL (unified):**
```python
# RedisVL (same Redis connection as everything else)
from redisvl.index import SearchIndex
from redisvl.query import VectorQuery

index = SearchIndex(schema=procedural_schema)
index.connect(redis_url)  # Same Redis as conversations

results = index.query(VectorQuery(vector=embedding, num_results=3))
# Everything local, no cloud dependencies
```

#### 3. Data Locality = Speed

**Problem:** Multi-system architectures have network overhead:

```
User query ‚Üí Backend ‚Üí PostgreSQL (50ms)
                    ‚Üí Pinecone API (200ms)
                    ‚Üí Redis cache (2ms)
Total: 252ms just for data fetching
```

**Redis Stack (in-memory):**
```
User query ‚Üí Backend ‚Üí Redis (0.5ms for all data)
Total: 0.5ms
```

**500x faster data access** because everything is in-memory, local, and in one system.

#### 4. Simple Deployment

**Dockerfile:**
```dockerfile
FROM redis/redis-stack:latest
# That's it. All features included.
```

**vs. Multi-system:**
```yaml
# docker-compose.yml
services:
  postgres:
    image: postgres:15
    # + pgvector extension setup
  redis:
    image: redis:7
  vector-db:
    # Qdrant or similar
  backend:
    # Must connect to all 3 services
```

**One container vs three** = simpler debugging, fewer failure points, faster startup.

### What We Gave Up

**Drawbacks of Redis:**

1. **No SQL queries** - Must design keys carefully
   - **Mitigation:** Use clear key patterns (see `redis_keys.py`)

2. **In-memory = data loss risk** - If Redis crashes, data is gone
   - **Mitigation:** Enable RDB snapshots (automatic backups)
   - **Mitigation:** Acceptable for demo (health data can be re-imported)

3. **No complex joins** - Can't do `SELECT * FROM workouts JOIN metrics ON ...`
   - **Mitigation:** Denormalize data (store workout details in HASH)

4. **Learning curve** - Developers familiar with SQL must learn Redis data structures
   - **Mitigation:** This is a teaching demo - learning Redis is valuable

**Conclusion:** For AI agent workloads, the speed and simplicity outweigh the tradeoffs.

## Decision 2: Why LangGraph?

### The Choice

**LangGraph** = State machine framework for agentic workflows

### Alternatives Considered

| Option | Pros | Cons | Verdict |
|--------|------|------|---------|
| **LCEL (LangChain Expression Language)** | Simple, functional | Hard to debug, no checkpointing, implicit flow | ‚ùå Too opaque |
| **Raw LangChain Agent** | Built-in, easy start | Black box, limited control | ‚ùå No visibility |
| **CrewAI** | Multi-agent focus | Overkill for single agent, complex | ‚ùå Too complex |
| **LangGraph** | Explicit state machine, checkpointing, debuggable | More verbose | ‚úÖ **Winner** |

### Why LangGraph Won

#### 1. Explicit State Machine (vs Implicit LCEL)

**LCEL (bad for complex workflows):**
```python
# LCEL chain - looks simple, hard to debug
chain = (
    RunnablePassthrough()
    | llm.bind_tools(tools)
    | RunnableLambda(execute_tools)
    | llm
)

# What happened when it failed?
# Which stage broke?
# Can't inject custom logic between stages
```

**LangGraph (good for complex workflows):**
```python
# LangGraph - explicit nodes and edges
graph = StateGraph(MemoryState)

graph.add_node("retrieve_memory", retrieve_memory_node)
graph.add_node("llm", llm_node)
graph.add_node("tools", tool_node)
graph.add_node("store_memory", store_memory_node)

graph.set_entry_point("retrieve_memory")
graph.add_edge("retrieve_memory", "llm")
graph.add_conditional_edges("llm", should_continue, {
    "tools": "tools",
    "end": "store_memory"
})
graph.add_edge("tools", "llm")
graph.add_edge("store_memory", END)

compiled = graph.compile(checkpointer=checkpointer)
```

**Benefits:**
- **Debuggable:** See exactly which node failed
- **Extensible:** Add validation, refinement, or logging nodes easily
- **Testable:** Test each node in isolation

#### 2. Built-In Checkpointing

**LangGraph checkpointing** = automatic conversation persistence

**Without checkpointing (manual):**
```python
# Must manually save conversation after each turn
messages = load_conversation(session_id)
messages.append(HumanMessage(content=user_query))
response = llm.invoke(messages)
messages.append(AIMessage(content=response.content))
save_conversation(session_id, messages)  # Manual save
```

**With LangGraph checkpointing:**
```python
# Checkpointer handles save/load automatically
checkpointer = AsyncRedisSaver(redis_url)
graph = workflow.compile(checkpointer=checkpointer)

# Conversation saved automatically after each node
final_state = await graph.ainvoke(
    input_state,
    config={"configurable": {"thread_id": session_id}}
)
# No manual save needed!
```

**What checkpointing gives us:**
- **Automatic persistence:** Saves conversation after each step
- **Thread isolation:** Multiple sessions don't interfere
- **Resume from failure:** If agent crashes, can resume from last checkpoint
- **Time travel:** Can inspect state at any point in graph execution

#### 3. Conditional Edges (Tool Looping)

**LangGraph conditional edges** = dynamic routing between nodes

**Example: Tool looping until done:**
```python
def should_continue(state: MemoryState) -> str:
    """Decide if we need to call more tools."""
    last_msg = state["messages"][-1]
    has_tool_calls = hasattr(last_msg, "tool_calls") and last_msg.tool_calls
    return "tools" if has_tool_calls else "end"

graph.add_conditional_edges("llm", should_continue, {
    "tools": "tools",
    "end": "store_memory"
})
```

**Flow:**
```
llm ‚Üí has tool_calls? ‚Üí YES ‚Üí tools ‚Üí llm (loop)
                      ‚Üí NO  ‚Üí store_memory ‚Üí END
```

**Why this matters:**
Multi-step queries like "Compare my workouts this week vs last week" might need:
1. Call `compare_activity` tool
2. Call `search_workouts` tool
3. Call `aggregate_metrics` tool
4. Synthesize final response

LangGraph handles this loop automatically.

### What We Gave Up

**Drawbacks of LangGraph:**

1. **More verbose** than LCEL - More code to write
   - **Mitigation:** Explicit is better than implicit for teaching

2. **Learning curve** - Need to understand state machines
   - **Mitigation:** This is a teaching demo - valuable concept

3. **Newer library** - Less mature than core LangChain
   - **Mitigation:** Strong community support, active development

**Conclusion:** For agentic workflows with memory and tool chaining, LangGraph's explicitness is worth the verbosity.

## Decision 3: Why Qwen 2.5 7B?

### The Choice

**Qwen 2.5 7B** = 4.7 GB parameter model optimized for function calling

### Alternatives Considered

| Model | Size | Function Calling | Speed | Privacy | Verdict |
|-------|------|------------------|-------|---------|---------|
| **GPT-4** | Cloud | ‚úÖ Excellent | Slow | ‚ùå Cloud | ‚ùå No privacy |
| **Llama 3.1 8B** | 4.9 GB | ‚ö†Ô∏è Okay | Fast | ‚úÖ Local | ‚ö†Ô∏è Worse tool calling |
| **Mistral 7B** | 4.1 GB | ‚ùå Poor | Fast | ‚úÖ Local | ‚ùå Bad tool selection |
| **Qwen 2.5 7B** | 4.7 GB | ‚úÖ Excellent | Fast | ‚úÖ Local | ‚úÖ **Winner** |

### Why Qwen 2.5 7B Won

#### 1. Native Function Calling Support

**Qwen 2.5 training:** Specifically trained on function-calling datasets

**What this means:**
- **Consistent JSON formatting:** Reliably returns valid tool arguments
- **Smart tool selection:** Chooses the right tool for natural language queries
- **Multi-tool chaining:** Knows when to call multiple tools sequentially

**Example: Tool selection accuracy**

**User:** "What day do I consistently push my heart rate when I work out?"

**Qwen 2.5 7B:**
```json
{
    "name": "analyze_workout_intensity_by_day",
    "args": {}
}
```
‚úÖ Correct tool on first try

**Llama 3.1 8B:**
```json
{
    "name": "search_workouts_and_activity",
    "args": {"days_back": 30}
}
```
‚ö†Ô∏è Returns raw data, needs second call to analyze

**Mistral 7B:**
```json
{
    "name": "search_health_records_by_metric",
    "args": {"metric_types": ["HeartRate"]}
}
```
‚ùå Wrong tool (health records, not workout patterns)

#### 2. Reasonable Size (Runs on Most Laptops)

**Size comparison:**

| Model | Download Size | RAM Needed | Runs On |
|-------|---------------|------------|---------|
| GPT-4 | N/A (cloud) | N/A | ‚òÅÔ∏è Cloud only |
| Llama 3.1 70B | 40 GB | 64 GB+ RAM | üñ•Ô∏è High-end workstations |
| Qwen 2.5 7B | 4.7 GB | 8 GB+ RAM | üíª M1 Mac, modern PC |

**Target:** Most developers have 8-16 GB RAM laptops.

**Qwen 2.5 7B** fits comfortably in 8 GB, leaving room for Redis, frontend, etc.

#### 3. Ollama Support (One-Command Install)

**Ollama** = Local LLM runtime (like Docker for models)

**Installation:**
```bash
# Install Ollama
brew install ollama

# Pull Qwen 2.5 7B
ollama pull qwen2.5:7b

# Done. No API keys, no cloud accounts, no configuration.
```

**vs. GPT-4:**
```bash
# 1. Sign up for OpenAI account
# 2. Add credit card
# 3. Generate API key
# 4. Set environment variable
# 5. Hope you don't exceed rate limits
```

**Ollama advantages:**
- **One-command install:** No account, no API key
- **Automatic model management:** Handles downloads, updates, cleanup
- **Simple API:** Same interface as OpenAI (drop-in replacement)
- **Local-first:** Works offline, no rate limits

### What We Gave Up

**Drawbacks of Qwen 2.5 7B:**

1. **Not as smart as GPT-4** - Worse at complex reasoning
   - **Mitigation:** Health queries are straightforward (tool selection, data lookup)

2. **Slower than cloud GPT-4** - 2-5 seconds vs <1 second
   - **Mitigation:** Acceptable for demo (privacy > speed)

3. **Requires powerful laptop** - Won't run on old hardware
   - **Mitigation:** Target developers with modern machines

**Conclusion:** For privacy-first health demos, Qwen 2.5 7B is the sweet spot (local, fast enough, good tool calling).

## Decision 4: Why Ollama?

### The Choice

**Ollama** = Local LLM runtime (handles model loading, inference, API)

### Alternatives Considered

| Option | Pros | Cons | Verdict |
|--------|------|------|---------|
| **llama.cpp** | Lightweight, fast | Manual model loading, no API | ‚ùå Too low-level |
| **vLLM** | Production-grade | Complex setup, GPU-focused | ‚ùå Overkill |
| **HuggingFace Transformers** | Flexible | Must write inference code | ‚ùå Too much work |
| **OpenAI API** | Best performance | Cloud-only, costs money | ‚ùå No privacy |
| **Ollama** | Simple API, handles everything | Less control | ‚úÖ **Winner** |

### Why Ollama Won

#### 1. Simple API (OpenAI-Compatible)

**Ollama API** = Drop-in replacement for OpenAI

**Code:**
```python
from langchain_ollama import ChatOllama

# Same API as OpenAI's ChatOpenAI
llm = ChatOllama(
    model="qwen2.5:7b",
    base_url="http://localhost:11434",
    temperature=0.05
)

# Works exactly like OpenAI
response = await llm.ainvoke(messages)
```

**Switch to OpenAI:**
```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    model="gpt-4",
    api_key="...",
    temperature=0.05
)
# Same code!
```

**Benefit:** Can prototype locally (Ollama), deploy to cloud (OpenAI) without rewriting code.

#### 2. Handles Model Loading Automatically

**Without Ollama (manual):**
```python
# Must manually download model, load into memory, manage GPU
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")

# Must handle tokenization, generation, decoding manually
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_length=1024)
response = tokenizer.decode(outputs[0])
```

**With Ollama (automatic):**
```bash
ollama pull qwen2.5:7b
# Ollama handles download, GPU detection, memory management
```

```python
llm = ChatOllama(model="qwen2.5:7b")
response = await llm.ainvoke(messages)
# Ollama handles tokenization, generation, decoding
```

**Ollama abstracts away:**
- Model downloading and caching
- GPU detection and optimization
- Memory management
- Tokenization and generation
- Concurrent request handling

#### 3. Works Offline (No API Keys)

**Privacy guarantee:**
```python
# With Ollama (100% local)
llm = ChatOllama(model="qwen2.5:7b", base_url="http://localhost:11434")
# Health data NEVER leaves your machine

# With OpenAI (cloud)
llm = ChatOpenAI(model="gpt-4", api_key="...")
# Every query sent to OpenAI servers (privacy risk)
```

**For health data**, offline-first is critical.

### What We Gave Up

**Drawbacks of Ollama:**

1. **Less control** than llama.cpp or HuggingFace
   - **Mitigation:** Fine for demo (not production research)

2. **No advanced features** (speculative decoding, quantization tuning)
   - **Mitigation:** Ollama handles sensible defaults

3. **Local hardware requirement** - Can't scale to millions of users
   - **Mitigation:** Demo is for developers to run locally

**Conclusion:** For local-first demos, Ollama's simplicity beats lower-level control.

## What We Removed (and Why)

### Removed: Query Classifier

**What it was:** Pre-LLM step to categorize queries

```python
def classify_query(query: str) -> str:
    if "weight" in query:
        return "weight_query"
    elif "workout" in query:
        return "workout_query"
    # ...hundreds of rules
```

**Why we removed it:**
- **Qwen 2.5 7B already does this** (via tool selection)
- **Redundant complexity** (two layers of classification)
- **Brittle rules** (breaks with variations)

**Lesson:** Don't add pre-LLM logic that the LLM can handle autonomously.

### Removed: Episodic/Semantic Memory (Sparse Data)

**What it was:** Store every conversation as "episodes" for semantic search

**Why we removed it:**
- **Too sparse:** Demo conversations are 3-5 messages (not enough history)
- **Redundant:** Short-term memory (checkpointing) already handles this
- **Over-engineering:** Added complexity without value

**Lesson:** Only add memory types that have enough data to be useful.

### Removed: Complex Refinement

**What it was:** Multi-pass response refinement (validate ‚Üí refine ‚Üí re-validate)

**Why we removed it:**
- **Too slow:** Added 2-3 seconds per query
- **Diminishing returns:** First response was usually good enough
- **Complexity:** Hard to debug multi-pass workflows

**Lesson:** Simple and fast > complex and perfect (for demos).

## Architecture Summary

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ User                                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Frontend (TypeScript)                       ‚îÇ
‚îÇ - Side-by-side chat UI                      ‚îÇ
‚îÇ - Real-time streaming (SSE)                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
                ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Backend (FastAPI)                           ‚îÇ
‚îÇ - Chat endpoints (/api/chat/redis)          ‚îÇ
‚îÇ - Tool execution                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚Üì               ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Redis Stack  ‚îÇ ‚îÇ Ollama (Local LLM)          ‚îÇ
‚îÇ - LIST       ‚îÇ ‚îÇ - Qwen 2.5 7B               ‚îÇ
‚îÇ - HASH       ‚îÇ ‚îÇ - mxbai-embed-large         ‚îÇ
‚îÇ - ZSET       ‚îÇ ‚îÇ - No cloud, no API keys     ‚îÇ
‚îÇ - Vector     ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚Üë
        ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ LangGraph Agent                            ‚îÇ
‚îÇ 1. Retrieve memory (Redis LIST)            ‚îÇ
‚îÇ 2. Retrieve procedural patterns (Vector)   ‚îÇ
‚îÇ 3. LLM decides tools (Qwen 2.5)            ‚îÇ
‚îÇ 4. Execute tools (Redis queries)           ‚îÇ
‚îÇ 5. Store memories (Redis + Vector)         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Key Takeaways

1. **Redis Stack = unified data layer** - Conversations, health data, vectors in one system
2. **LangGraph = explicit workflow control** - State machines beat black-box chains
3. **Qwen 2.5 7B = local + smart tool calling** - Privacy + good function calling
4. **Ollama = developer experience** - One-command install, no API keys
5. **Local-first = privacy guarantee** - Health data never leaves your machine
6. **Simplicity > features** - Removed classifier, complex refinement, sparse memory

## Next Steps

- **07_APPLE_HEALTH_DATA.md** - Import and parse Apple Health exports
- **08_EXTENDING.md** - Add new tools, data sources, and deploy for production
