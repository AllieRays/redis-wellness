"""
Minimal LangGraph agent - NO memory, NO checkpointing, JUST tools.
This is the baseline to prove LangGraph works before adding complexity.
"""

import logging
from typing import Annotated, Any

from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage, ToolMessage
from langgraph.graph import END, StateGraph
from langgraph.graph.message import add_messages
from typing_extensions import TypedDict  # Use typing_extensions for Python 3.11
from langgraph.checkpoint.base import BaseCheckpointSaver

from ..apple_health.query_tools import create_user_bound_tools
from ..utils.agent_helpers import build_base_system_prompt, create_health_llm
from ..utils.numeric_validator import get_numeric_validator

logger = logging.getLogger(__name__)


class MinimalState(TypedDict):
    """State with messages, user ID, session ID, and memory context."""
    messages: Annotated[list[BaseMessage], add_messages]
    user_id: str
    session_id: str | None
    episodic_memory: str | None  # Retrieved episodic context


class StatefulRAGAgent:
    """LangGraph-based stateful agent with CoALA memory system."""

    def __init__(self, checkpointer: BaseCheckpointSaver | None = None, memory_coordinator = None):
        self.llm = create_health_llm()
        self.checkpointer = checkpointer
        self.memory = memory_coordinator
        self.graph = self._build_graph()

        if checkpointer and memory_coordinator:
            logger.info("âœ… StatefulRAGAgent initialized WITH checkpointing AND memory")
        elif checkpointer:
            logger.info("âœ… StatefulRAGAgent initialized WITH checkpointing (no memory)")
        else:
            logger.info("âœ… StatefulRAGAgent initialized (no checkpointing, no memory)")

    def _build_graph(self):
        """Build graph with memory: Retrieve Memory â†’ LLM â†’ Tools â†’ Store Memory â†’ End."""
        workflow = StateGraph(MinimalState)

        # Add memory nodes (if memory coordinator exists)
        if self.memory:
            workflow.add_node("retrieve_memory", self._retrieve_memory_node)
            workflow.add_node("store_memory", self._store_memory_node)

        workflow.add_node("llm", self._llm_node)
        workflow.add_node("tools", self._tool_node)

        # Set entry point based on memory availability
        if self.memory:
            workflow.set_entry_point("retrieve_memory")
            workflow.add_edge("retrieve_memory", "llm")
        else:
            workflow.set_entry_point("llm")

        # LLM â†’ Tools (if needed) or End
        workflow.add_conditional_edges("llm", self._should_continue, {"tools": "tools", "end": END})

        # Tools â†’ LLM (loop) or Store Memory
        if self.memory:
            workflow.add_edge("tools", "llm")
            # After LLM finishes (no more tools), store memory
            # Note: This is simplified - in production you'd want a cleaner "done" node
        else:
            workflow.add_edge("tools", "llm")

        # Compile with checkpointer if provided
        return workflow.compile(checkpointer=self.checkpointer)

    async def _retrieve_memory_node(self, state: MinimalState) -> dict:
        """Retrieve episodic memory before LLM call."""
        if not self.memory:
            return {"episodic_memory": None}

        logger.info("ğŸ§  Retrieving episodic memory...")

        # Get user's last message as query
        user_message = state["messages"][-1].content if state["messages"] else ""
        user_id = state["user_id"]
        session_id = state.get("session_id", "default")

        try:
            # Retrieve all memory context (episodic, semantic, procedural, short-term)
            memory_context = await self.memory.retrieve_all_context(
                session_id=session_id,
                query=user_message,
                include_procedural=False,  # Don't need procedural for now
                include_semantic=False,  # Don't need semantic for now
            )

            episodic_text = memory_context.episodic
            if episodic_text:
                logger.info(f"âœ… Retrieved episodic memory: {len(episodic_text)} chars, {memory_context.episodic_hits} hits")
            else:
                logger.info("â„¹ï¸ No episodic memories found")

            return {"episodic_memory": episodic_text}

        except Exception as e:
            logger.error(f"âŒ Memory retrieval failed: {e}")
            return {"episodic_memory": None}

    async def _store_memory_node(self, state: MinimalState) -> dict:
        """Store interaction in episodic memory after LLM response."""
        if not self.memory:
            return {}

        logger.info("ğŸ’¾ Storing interaction in memory...")

        try:
            # Extract user message and assistant response
            user_msg = None
            assistant_msg = None
            for msg in reversed(state["messages"]):
                if isinstance(msg, AIMessage) and not assistant_msg:
                    assistant_msg = msg.content
                elif isinstance(msg, HumanMessage) and not user_msg:
                    user_msg = msg.content
                if user_msg and assistant_msg:
                    break

            if user_msg and assistant_msg:
                session_id = state.get("session_id", "default")

                # Store interaction in all memory systems
                await self.memory.store_interaction(
                    session_id=session_id,
                    user_message=user_msg,
                    assistant_response=assistant_msg,
                    tools_used=[],  # Extract from ToolMessage if needed
                )

                logger.info("âœ… Interaction stored in memory")

        except Exception as e:
            logger.error(f"âŒ Memory storage failed: {e}")

        return {}

    async def _llm_node(self, state: MinimalState) -> dict:
        """Call LLM with tools and memory context."""
        logger.info("ğŸ¤– LLM node")
        logger.info(f"   State has {len(state['messages'])} messages")

        # Log message types for debugging
        for i, msg in enumerate(state["messages"]):
            msg_type = type(msg).__name__
            content_preview = str(msg.content)[:80] if hasattr(msg, 'content') else "no content"
            logger.info(f"   [{i}] {msg_type}: {content_preview}")

        # Build system prompt with episodic memory context
        system_prompt = build_base_system_prompt()

        # Inject episodic memory if available
        if state.get("episodic_memory"):
            system_prompt += f"\n\nğŸ“‹ USER CONTEXT (from memory):\n{state['episodic_memory']}"
            logger.info("âœ… Injected episodic memory into prompt")

        # Bind tools
        tools = create_user_bound_tools(state["user_id"], conversation_history=state["messages"])
        llm_with_tools = self.llm.bind_tools(tools)

        # Call LLM
        messages = [SystemMessage(content=system_prompt)] + state["messages"]
        logger.info(f"   Calling LLM with {len(messages)} total messages (system + {len(state['messages'])} history)")
        response = await llm_with_tools.ainvoke(messages)

        logger.info(f"LLM called tools: {bool(getattr(response, 'tool_calls', None))}")
        return {"messages": [response]}

    async def _tool_node(self, state: MinimalState) -> dict:
        """Execute tools."""
        last_msg = state["messages"][-1]
        tool_calls = getattr(last_msg, "tool_calls", [])

        logger.info(f"ğŸ”§ Executing {len(tool_calls)} tools")

        tools = create_user_bound_tools(state["user_id"], conversation_history=state["messages"])
        tool_messages = []

        for tool_call in tool_calls:
            tool_name = tool_call.get("name")
            logger.info(f"   â†’ {tool_name}")

            for tool in tools:
                if tool.name == tool_name:
                    result = await tool.ainvoke(tool_call["args"])
                    tool_messages.append(
                        ToolMessage(
                            content=str(result),
                            tool_call_id=tool_call.get("id", ""),
                            name=tool_name
                        )
                    )
                    break

        return {"messages": tool_messages}

    def _should_continue(self, state: MinimalState) -> str:
        """Check if we need to call tools."""
        last_msg = state["messages"][-1]
        if hasattr(last_msg, "tool_calls") and last_msg.tool_calls:
            return "tools"
        return "end"

    async def chat(self, message: str, user_id: str, session_id: str = "default") -> dict[str, Any]:
        """Process message through graph."""
        logger.info(f"ğŸ¯ StatefulRAGAgent.chat() called: message='{message[:50]}...', session_id={session_id}")
        input_state = {
            "messages": [HumanMessage(content=message)],
            "user_id": user_id,
            "session_id": session_id,
            "episodic_memory": None
        }

        # Add config for checkpointing
        config = {"configurable": {"thread_id": session_id}} if self.checkpointer else None

        if config:
            logger.info(f"ğŸ“ Using checkpoint thread_id: {session_id}")

        final_state = await self.graph.ainvoke(input_state, config)
        logger.info(f"ğŸ“Š Final state has {len(final_state['messages'])} messages")

        # Extract response from final message
        response_text = final_state["messages"][-1].content

        # Extract tools used and tool results
        tools_used = []
        tool_results = []
        for msg in final_state["messages"]:
            if isinstance(msg, ToolMessage):
                tools_used.append(msg.name)
                tool_results.append({"name": msg.name, "content": msg.content})

        # Validate response for numeric hallucinations
        validator = get_numeric_validator()
        validation_result = validator.validate_response(
            response_text=response_text,
            tool_results=tool_results,
            strict=False,
        )

        # Log validation results
        if not validation_result["valid"]:
            logger.warning(
                f"âš ï¸ Validation failed (score: {validation_result['score']:.2%}) - "
                f"Hallucinations detected: {len(validation_result.get('hallucinations', []))}"
            )
            for hallucination in validation_result.get("hallucinations", []):
                logger.warning(f"   Hallucinated number: {hallucination}")
        else:
            logger.info(
                f"âœ… Validation passed (score: {validation_result['score']:.2%})"
            )

        return {
            "response": response_text,
            "tools_used": list(set(tools_used)),  # Deduplicate
            "tool_calls_made": len(tools_used),
            "validation": {
                "valid": validation_result["valid"],
                "score": validation_result["score"],
                "hallucinations_detected": len(validation_result.get("hallucinations", [])),
                "numbers_validated": validation_result.get("stats", {}).get("matched", 0),
                "total_numbers": validation_result.get("stats", {}).get("total_numbers", 0),
            }
        }

    async def chat_stream(self, message: str, user_id: str, session_id: str = "default"):
        """Stream tokens through graph (simplified - just return full response for now)."""
        # For Phase 1, just use non-streaming and yield the result
        # We'll add proper streaming in later phases
        result = await self.chat(message, user_id, session_id)

        # Yield the response as if it were streaming
        yield {"type": "token", "content": result["response"]}

        # Yield done event
        yield {
            "type": "done",
            "data": {
                "response": result["response"],
                "tools_used": result["tools_used"],
                "tool_calls_made": result["tool_calls_made"]
            }
        }
