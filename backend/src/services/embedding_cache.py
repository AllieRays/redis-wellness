"""
Embedding Cache Service.

Caches embeddings generated by Ollama to avoid expensive recomputation.
Provides significant performance improvement for repeated or similar queries.

Performance Impact:
- Without cache: ~200ms per embedding generation (Ollama inference)
- With cache hit: <1ms (Redis GET operation)
- Typical hit rate: 30-50% in production

Architecture:
- Service layer (not util) because it handles I/O and state
- Uses RedisConnectionManager for connection pooling
- TTL-based expiration (default 1 hour)
- Tracks cache statistics (hits/misses/hit rate)
"""

import hashlib
import json
import logging
from typing import Any

from .redis_connection import get_redis_manager

logger = logging.getLogger(__name__)


class EmbeddingCache:
    """
    Redis-backed embedding cache service.

    Caches embedding vectors to avoid regenerating them on repeated queries.
    Each cache entry has a TTL (time-to-live) for automatic expiration.

    Cache Key Format:
        embedding_cache:{query_hash}

    Example:
        cache = EmbeddingCache(ttl_seconds=3600)

        # First call: cache miss (generates embedding)
        embedding = await cache.get_or_generate(
            query="What is my BMI?",
            generate_fn=lambda: ollama_generate_embedding("What is my BMI?")
        )

        # Second call: cache hit (returns cached embedding)
        embedding = await cache.get_or_generate(
            query="What is my BMI?",
            generate_fn=...  # Not called!
        )
    """

    def __init__(self, ttl_seconds: int = 3600, key_prefix: str = "embedding_cache"):
        """
        Initialize embedding cache.

        Args:
            ttl_seconds: Time-to-live for cache entries (default 1 hour)
            key_prefix: Redis key prefix for cache entries
        """
        self.ttl_seconds = ttl_seconds
        self.key_prefix = key_prefix
        self.redis_manager = get_redis_manager()

        # Cache statistics
        self.stats = {"hits": 0, "misses": 0, "total_queries": 0, "hit_rate": 0.0}

        logger.info(
            f"EmbeddingCache initialized (TTL: {ttl_seconds}s, prefix: {key_prefix})"
        )

    def _generate_cache_key(self, query: str) -> str:
        """
        Generate cache key from query text.

        Uses MD5 hash to create deterministic key from query.
        Same query text always generates same cache key.

        Args:
            query: User query text

        Returns:
            Redis cache key (e.g., "embedding_cache:a1b2c3d4...")
        """
        # MD5 hash for compact, deterministic keys
        query_hash = hashlib.md5(query.encode("utf-8")).hexdigest()
        return f"{self.key_prefix}:{query_hash}"

    async def get(self, query: str) -> list[float] | None:
        """
        Get cached embedding for query.

        Args:
            query: User query text

        Returns:
            Cached embedding vector or None if not found
        """
        cache_key = self._generate_cache_key(query)

        try:
            with self.redis_manager.get_connection() as redis_client:
                cached_data = redis_client.get(cache_key)

                if cached_data:
                    embedding = json.loads(cached_data)
                    self._record_hit()
                    logger.debug(f"âœ… Cache HIT: {query[:50]}...")
                    return embedding

                self._record_miss()
                logger.debug(f"âŒ Cache MISS: {query[:50]}...")
                return None

        except Exception as e:
            logger.warning(f"Cache lookup failed: {e}")
            self._record_miss()
            return None

    async def set(self, query: str, embedding: list[float]) -> bool:
        """
        Store embedding in cache.

        Args:
            query: User query text
            embedding: Embedding vector to cache

        Returns:
            True if stored successfully, False otherwise
        """
        cache_key = self._generate_cache_key(query)

        try:
            with self.redis_manager.get_connection() as redis_client:
                # Store with TTL
                redis_client.setex(cache_key, self.ttl_seconds, json.dumps(embedding))

                logger.debug(
                    f"ðŸ’¾ Cached embedding: {query[:50]}... (TTL: {self.ttl_seconds}s)"
                )
                return True

        except Exception as e:
            logger.error(f"Cache store failed: {e}")
            return False

    async def get_or_generate(
        self, query: str, generate_fn: callable
    ) -> list[float] | None:
        """
        Get cached embedding or generate new one.

        This is the primary method to use. It handles cache lookup,
        generation on miss, and storing the result.

        Args:
            query: User query text
            generate_fn: Async function to generate embedding (called on cache miss)

        Returns:
            Embedding vector (from cache or freshly generated)

        Example:
            embedding = await cache.get_or_generate(
                query="What is my BMI?",
                generate_fn=lambda: self._generate_embedding("What is my BMI?")
            )
        """
        # Try cache first
        cached_embedding = await self.get(query)
        if cached_embedding is not None:
            return cached_embedding

        # Cache miss: generate fresh embedding
        try:
            embedding = await generate_fn()

            if embedding:
                # Store in cache for next time
                await self.set(query, embedding)

            return embedding

        except Exception as e:
            logger.error(f"Embedding generation failed: {e}")
            return None

    def _record_hit(self):
        """Record cache hit and update statistics."""
        self.stats["hits"] += 1
        self.stats["total_queries"] += 1
        self._update_hit_rate()

    def _record_miss(self):
        """Record cache miss and update statistics."""
        self.stats["misses"] += 1
        self.stats["total_queries"] += 1
        self._update_hit_rate()

    def _update_hit_rate(self):
        """Recalculate cache hit rate."""
        if self.stats["total_queries"] > 0:
            self.stats["hit_rate"] = self.stats["hits"] / self.stats["total_queries"]

    def get_stats(self) -> dict[str, Any]:
        """
        Get cache performance statistics.

        Returns:
            Dict with hits, misses, hit_rate, estimated_time_saved_ms
        """
        # Estimate time saved (200ms per cache hit)
        time_saved_ms = self.stats["hits"] * 200

        return {
            "hits": self.stats["hits"],
            "misses": self.stats["misses"],
            "total_queries": self.stats["total_queries"],
            "hit_rate": f"{self.stats['hit_rate']:.2%}",
            "hit_rate_float": self.stats["hit_rate"],
            "estimated_time_saved_ms": time_saved_ms,
            "estimated_time_saved_seconds": time_saved_ms / 1000,
            "cache_ttl_seconds": self.ttl_seconds,
        }

    async def clear(self) -> int:
        """
        Clear all cached embeddings.

        Returns:
            Number of keys deleted

        Warning:
            This deletes ALL keys with the cache prefix.
            Use cautiously in production.
        """
        try:
            with self.redis_manager.get_connection() as redis_client:
                pattern = f"{self.key_prefix}:*"
                cursor = 0
                deleted = 0

                # Scan and delete in batches
                while True:
                    cursor, keys = redis_client.scan(cursor, match=pattern, count=100)

                    if keys:
                        redis_client.delete(*keys)
                        deleted += len(keys)

                    if cursor == 0:
                        break

                logger.info(f"Cleared {deleted} cached embeddings")
                return deleted

        except Exception as e:
            logger.error(f"Cache clear failed: {e}")
            return 0


# Global embedding cache instance
_embedding_cache: EmbeddingCache | None = None


def get_embedding_cache(ttl_seconds: int = 3600) -> EmbeddingCache:
    """
    Get or create global embedding cache instance.

    Args:
        ttl_seconds: TTL for cache entries (default 1 hour)

    Returns:
        EmbeddingCache instance
    """
    global _embedding_cache

    if _embedding_cache is None:
        _embedding_cache = EmbeddingCache(ttl_seconds=ttl_seconds)

    return _embedding_cache
